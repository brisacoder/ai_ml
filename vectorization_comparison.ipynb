{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Method Comparison on Sentiment Detection\n",
    "\n",
    "We test how each vectorization method performs on sentiment detection using a small labeled corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentiment-labeled corpus\n",
    "sent_corpus = [\n",
    "    \"I love this movie\",\n",
    "    \"This film was fantastic\",\n",
    "    \"I hated this movie\",\n",
    "    \"This film was terrible\"\n",
    "]\n",
    "sent_labels = [1, 1, 0, 0]  # 1 = Positive, 0 = Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Bag of Words and TF-IDF Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# BoW\n",
    "cv_bow = CountVectorizer()\n",
    "X_bow = cv_bow.fit_transform(sent_corpus)\n",
    "clf_bow = LogisticRegression().fit(X_bow, sent_labels)\n",
    "bow_preds = clf_bow.predict(X_bow)\n",
    "print(\"BoW Accuracy:\", accuracy_score(sent_labels, bow_preds))\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vec.fit_transform(sent_corpus)\n",
    "clf_tfidf = LogisticRegression().fit(X_tfidf, sent_labels)\n",
    "tfidf_preds = clf_tfidf.predict(X_tfidf)\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(sent_labels, tfidf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Word2Vec Performance (Averaged Word Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# import numpy as np\n",
    "\n",
    "# tokenized_sent = [s.lower().split() for s in sent_corpus]\n",
    "# w2v_model = Word2Vec(sentences=tokenized_sent, vector_size=50, window=2, min_count=1, seed=1)\n",
    "\n",
    "# def average_vector(sentence, model):\n",
    "#     tokens = sentence.lower().split()\n",
    "#     vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "#     return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# X_w2v = np.array([average_vector(s, w2v_model) for s in sent_corpus])\n",
    "# clf_w2v = LogisticRegression().fit(X_w2v, sent_labels)\n",
    "# w2v_preds = clf_w2v.predict(X_w2v)\n",
    "# print(\"Word2Vec Accuracy:\", accuracy_score(sent_labels, w2v_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Observations\n",
    "\n",
    "- **BoW** works well when training and test texts share exact terms.\n",
    "- **TF-IDF** improves weighting for rare but informative words.\n",
    "- **Word2Vec** captures semantic relationships (e.g., \"love\" and \"fantastic\" are both positive), making it better for generalization.\n",
    "\n",
    "This illustrates that **context-aware embeddings like Word2Vec** are more robust to variation in wording, while **BoW/TF-IDF** are literal and vocabulary-dependent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
