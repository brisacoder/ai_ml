{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd16 BERTScore Demonstration\n", "\n", "This notebook shows how to compute **BERTScore** using Hugging Face's `evaluate` library.\n", "\n", "BERTScore compares model-generated text against reference text using contextual embeddings from BERT models, measuring:\n", "- **Precision**: similarity of generated tokens to reference tokens\n", "- **Recall**: how much of the reference is captured in the generated text\n", "- **F1-score**: harmonic mean of precision and recall"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install the evaluate library if needed\n", "# !pip install evaluate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import evaluate\n", "bertscore = evaluate.load(\"bertscore\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sample data: model prediction vs reference\n", "predictions = [\n", "    \"The cat sat on the mat.\"\n", "]\n", "\n", "references = [\n", "    \"A cat was sitting on a mat.\"\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute BERTScore using a base model (e.g., roberta-base)\n", "results = bertscore.compute(predictions=predictions, references=references, model_type=\"roberta-base\")\n", "results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83e\udde0 Notes:\n", "- BERTScore uses contextual embeddings instead of surface n-gram overlap.\n", "- It works well for evaluating **semantic similarity**, especially in generation tasks like summarization and translation.\n", "- You can choose different `model_type` (e.g., `bert-base-uncased`, `roberta-large`) for different trade-offs in quality vs speed."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}