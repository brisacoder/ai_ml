{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e5effd",
   "metadata": {},
   "source": [
    "# Self-Attention Next-Word Prediction with CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb136f",
   "metadata": {},
   "source": [
    "This notebook demonstrates a single self-attention head predicting the next word in a tiny vocabulary, **with training** and **CUDA** support if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597c5032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Choose device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d01032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define small vocabulary and mappings\n",
    "vocab = ['I', 'love', 'cats', 'dogs', '<pad>', '<eos>']\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d933f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy input: 'I love'\n",
    "input_words = ['I', 'love']\n",
    "input_idxs = torch.tensor([word_to_idx[w] for w in input_words], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6edfe81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters and embedding\n",
    "d_model, d_k, d_v = 8, 8, 8\n",
    "emb = torch.nn.Embedding(vocab_size, d_model).to(device)\n",
    "Wq = torch.nn.Linear(d_model, d_k).to(device)\n",
    "Wk = torch.nn.Linear(d_model, d_k).to(device)\n",
    "Wv = torch.nn.Linear(d_model, d_v).to(device)\n",
    "Wo = torch.nn.Linear(d_v, d_model).to(device)\n",
    "classifier = torch.nn.Linear(d_model, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3004d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(emb.parameters()) + list(Wq.parameters()) + list(Wk.parameters()) +\n",
    "    list(Wv.parameters()) + list(Wo.parameters()) + list(classifier.parameters()),\n",
    "    lr=0.01\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Single target: next word 'cats'\n",
    "targets = torch.tensor([word_to_idx['cats']], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4c1a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1.6159\n",
      "Epoch 50: loss 0.0000\n",
      "Epoch 100: loss 0.0000\n",
      "Epoch 150: loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    X = emb(input_idxs)        # (2, d_model)\n",
    "    Q = Wq(X); K = Wk(X); V = Wv(X)\n",
    "    scores = Q @ K.T / (d_k ** 0.5)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    context = attn @ V         # (2, d_v)\n",
    "    out = Wo(context)          # (2, d_model)\n",
    "    logits = classifier(out[-1]).unsqueeze(0)\n",
    "    loss = loss_fn(logits, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e1f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next-word probabilities after training:\n",
      "I: 0.0000\n",
      "love: 0.0000\n",
      "cats: 1.0000\n",
      "dogs: 0.0000\n",
      "<pad>: 0.0000\n",
      "<eos>: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate after training\n",
    "with torch.no_grad():\n",
    "    X = emb(input_idxs)\n",
    "    Q = Wq(X); K = Wk(X); V = Wv(X)\n",
    "    scores = Q @ K.T / (d_k ** 0.5)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    context = attn @ V\n",
    "    out = Wo(context)\n",
    "    logits = classifier(out[-1])\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print('Next-word probabilities after training:')\n",
    "    for i, p in enumerate(probs):\n",
    "        print(f\"{idx_to_word[i]}: {p.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
