{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udd24 BLEU Score Demonstration\n", "\n", "**BLEU** (Bilingual Evaluation Understudy) is a metric for evaluating the quality of machine-translated text compared to one or more reference translations.\n", "\n", "BLEU is based on:\n", "- n-gram precision\n", "- a brevity penalty for short hypotheses\n", "\n", "This notebook shows how to calculate BLEU using Hugging Face's `evaluate` library."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install if necessary\n", "# !pip install evaluate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import evaluate\n", "bleu = evaluate.load(\"bleu\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example: prediction vs reference translation\n", "predictions = [\n", "    \"The cat is on the mat.\"\n", "]\n", "\n", "references = [\n", "    [\"There is a cat sitting on the mat.\"]  # Note: BLEU expects list of lists for references\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute BLEU\n", "results = bleu.compute(predictions=predictions, references=references)\n", "results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83e\udde0 Notes:\n", "- BLEU uses **n-gram precision** (up to 4-grams by default).\n", "- It penalizes overly short translations via a **brevity penalty**.\n", "- Values range from 0 (no match) to 1 (perfect match).\n", "- Often used for evaluating **translation** and sometimes **summarization** (less ideal for abstractive cases)."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}