{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa99bcb",
   "metadata": {},
   "source": [
    "# Self-Attention Next-Word Prediction Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd9daa",
   "metadata": {},
   "source": [
    "This notebook demonstrates a single self-attention head predicting the next word in a tiny vocabulary, **with training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23498fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define small vocabulary and mappings\n",
    "vocab = ['I', 'love', 'cats', 'dogs', '<pad>', '<eos>']\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx_to_word = {i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy input: 'I love'\n",
    "input_words = ['I', 'love']\n",
    "input_idxs = torch.tensor([word_to_idx[w] for w in input_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters and embedding\n",
    "d_model, d_k, d_v = 8, 8, 8\n",
    "emb = torch.nn.Embedding(vocab_size, d_model)\n",
    "Wq = torch.nn.Linear(d_model, d_k)\n",
    "Wk = torch.nn.Linear(d_model, d_k)\n",
    "Wv = torch.nn.Linear(d_model, d_v)\n",
    "Wo = torch.nn.Linear(d_v, d_model)\n",
    "classifier = torch.nn.Linear(d_model, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78823bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(list(emb.parameters()) + \\\n",
    "    list(Wq.parameters()) + list(Wk.parameters()) + \\\n",
    "    list(Wv.parameters()) + list(Wo.parameters()) + \\\n",
    "    list(classifier.parameters()), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Single target: next word 'cats'\n",
    "targets = torch.tensor([word_to_idx['cats']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    X = emb(input_idxs)\n",
    "    Q = Wq(X); K = Wk(X); V = Wv(X)\n",
    "    scores = Q @ K.T / (d_k ** 0.5)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    context = attn @ V\n",
    "    out = Wo(context)\n",
    "    logits = classifier(out[-1]).unsqueeze(0)\n",
    "    loss = loss_fn(logits, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate after training\n",
    "with torch.no_grad():\n",
    "    X = emb(input_idxs)\n",
    "    Q = Wq(X); K = Wk(X); V = Wv(X)\n",
    "    scores = Q @ K.T / (d_k ** 0.5)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    context = attn @ V\n",
    "    out = Wo(context)\n",
    "    logits = classifier(out[-1])\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print('Next-word probabilities after training:')\n",
    "    for i, p in enumerate(probs):\n",
    "        print(f\"{idx_to_word[i]}: {p.item():.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
