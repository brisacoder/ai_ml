{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcca Classification Metrics: Precision, Recall, F1-score, and Specificity\n", "\n", "This notebook shows how to calculate and interpret classification metrics using a confusion matrix:\n", "\n", "- **Precision**\n", "- **Recall (Sensitivity)**\n", "- **Specificity**\n", "- **F1-score**\n", "\n", "We'll use a toy binary classification example."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example confusion matrix values\n", "# True Positives (TP), False Positives (FP), False Negatives (FN), True Negatives (TN)\n", "TP = 70\n", "FP = 10\n", "FN = 20\n", "TN = 100"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Precision: Of all predicted positives, how many are correct?\n", "precision = TP / (TP + FP)\n", "precision"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Recall (Sensitivity): Of all actual positives, how many did we catch?\n", "recall = TP / (TP + FN)\n", "recall"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Specificity: Of all actual negatives, how many did we correctly identify?\n", "specificity = TN / (TN + FP)\n", "specificity"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# F1 Score: Harmonic mean of precision and recall\n", "f1 = 2 * (precision * recall) / (precision + recall)\n", "f1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u2705 Summary of Formulas\n", "- **Precision** = TP / (TP + FP)\n", "- **Recall** = TP / (TP + FN)\n", "- **Specificity** = TN / (TN + FP)\n", "- **F1 Score** = 2 * (Precision * Recall) / (Precision + Recall)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}