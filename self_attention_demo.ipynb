{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac1b30d",
   "metadata": {},
   "source": [
    "# Self-Attention Next-Word Prediction Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5bf2d",
   "metadata": {},
   "source": [
    "This notebook demonstrates a single self-attention head predicting the next word in a tiny vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define small vocabulary and mappings\n",
    "vocab = ['I', 'love', 'cats', 'dogs', '<pad>', '<eos>']\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx_to_word = {i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e37d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy input: 'I love'\n",
    "input_words = ['I', 'love']\n",
    "input_idxs = torch.tensor([word_to_idx[w] for w in input_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb81ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters and embedding\n",
    "d_model = 8\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "emb = torch.nn.Embedding(vocab_size, d_model)\n",
    "Wq = torch.nn.Linear(d_model, d_k)\n",
    "Wk = torch.nn.Linear(d_model, d_k)\n",
    "Wv = torch.nn.Linear(d_model, d_v)\n",
    "Wo = torch.nn.Linear(d_v, d_model)\n",
    "classifier = torch.nn.Linear(d_model, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ceb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass for self-attention and next-word prediction\n",
    "X = emb(input_idxs)  # (2, d_model)\n",
    "Q = Wq(X)            # (2, d_k)\n",
    "K = Wk(X)\n",
    "V = Wv(X)\n",
    "scores = Q @ K.T / (d_k ** 0.5)\n",
    "attn = F.softmax(scores, dim=-1)\n",
    "context = attn @ V   # (2, d_v)\n",
    "out = Wo(context)    # (2, d_model)\n",
    "# Predict next word from the last position output\n",
    "logits = classifier(out[-1])\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print('Next-word probabilities:')\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"{idx_to_word[i]}: {p.item():.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
