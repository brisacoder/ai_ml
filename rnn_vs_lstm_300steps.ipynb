{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35200f6c",
   "metadata": {},
   "source": [
    "# ðŸ§  RNN vs LSTM: Long-Term Dependency with 300-Timestep Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdab129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91944fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data(n=1000, seq_len=300):\n",
    "    X = torch.randint(0, 2, (n, seq_len)).float()\n",
    "    y = X[:, 0]  # label is based on the first token only\n",
    "    return X.unsqueeze(-1), y\n",
    "\n",
    "X, y = generate_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(pred, label):\n",
    "    return ((pred > 0.5) == label).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=16, batch_first=True)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return torch.sigmoid(self.fc(out[:, -1, :]))\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=16, batch_first=True)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return torch.sigmoid(self.fc(out[:, -1, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, X, y, epochs=20):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    losses, accs = [], []\n",
    "    for _ in range(epochs):\n",
    "        pred = model(X).squeeze()\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = accuracy(pred, y)\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc)\n",
    "    return losses, accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc93e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_model = RNNModel()\n",
    "lstm_model = LSTMModel()\n",
    "rnn_losses, rnn_accs = train_model(rnn_model, X, y)\n",
    "lstm_losses, lstm_accs = train_model(lstm_model, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143feb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(rnn_losses, label='RNN Loss')\n",
    "plt.plot(lstm_losses, label='LSTM Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('RNN vs LSTM: Loss (Long-Term Dependency)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b219b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(rnn_accs, label='RNN Accuracy')\n",
    "plt.plot(lstm_accs, label='LSTM Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('RNN vs LSTM: Accuracy (Long-Term Dependency)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003bc888",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Conclusion\n",
    "\n",
    "- RNN struggles to remember the first token across 300 timesteps.\n",
    "- LSTM performs significantly better due to its long-term memory capability.\n",
    "- This demonstrates why LSTMs are preferred for tasks with long-range dependencies.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
