{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d64df65",
   "metadata": {},
   "source": [
    "# üß† Word2Vec from Scratch (Skip-gram, PyTorch Version)\n",
    "\n",
    "This notebook implements a minimal Word2Vec Skip-gram model without `gensim`, using PyTorch. It demonstrates training word embeddings on a small corpus with full visibility into the math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e904cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5e565",
   "metadata": {},
   "source": [
    "## üìÑ Toy Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf87b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"thank you very much\",\n",
    "    \"thank you for coming\",\n",
    "    \"you are welcome\",\n",
    "    \"thank you again\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdda499",
   "metadata": {},
   "source": [
    "## üî† Tokenize and Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a01aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize corpus\n",
    "tokenized = [sentence.split() for sentence in corpus]\n",
    "vocab = sorted(set(word for sentence in tokenized for word in sentence))\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary:\", word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50768d7",
   "metadata": {},
   "source": [
    "## üßæ Generate Training Data (Skip-gram pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "training_data = []\n",
    "\n",
    "for sentence in tokenized:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    for center_pos in range(len(indices)):\n",
    "        for offset in range(-window_size, window_size + 1):\n",
    "            context_pos = center_pos + offset\n",
    "            if context_pos < 0 or context_pos >= len(indices) or context_pos == center_pos:\n",
    "                continue\n",
    "            center = indices[center_pos]\n",
    "            context = indices[context_pos]\n",
    "            training_data.append((center, context))\n",
    "\n",
    "print(f\"Total training pairs: {len(training_data)}\\nExample:\", training_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b4749",
   "metadata": {},
   "source": [
    "## üß† Define Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_words):\n",
    "        return self.in_embed(center_words)\n",
    "\n",
    "    def predict(self, center_words):\n",
    "        center_vecs = self.in_embed(center_words)  # shape: (batch, embed_dim)\n",
    "        context_vecs = self.out_embed.weight       # shape: (vocab_size, embed_dim)\n",
    "        scores = torch.matmul(center_vecs, context_vecs.T)  # shape: (batch, vocab_size)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae3334",
   "metadata": {},
   "source": [
    "## üîß Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "model = Word2Vec(vocab_size, embedding_dim).cuda()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Convert data to tensors\n",
    "training_pairs = [(torch.tensor([c]).cuda(), torch.tensor([t]).cuda()) for c, t in training_data]\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    random.shuffle(training_pairs)\n",
    "    for center, target in training_pairs:\n",
    "        logits = model.predict(center)  # shape: (1, vocab_size)\n",
    "        loss = loss_fn(logits, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e4fe28",
   "metadata": {},
   "source": [
    "## üîç View Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1289c1",
   "metadata": {},
   "source": [
    "## üìä Visualize Word Embeddings (2D Projection using PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensions to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, word in idx2word.items():\n",
    "    x, y = reduced[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x + 0.01, y + 0.01, word, fontsize=12)\n",
    "plt.title(\"Word Embeddings (PCA Projection)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.in_embed.weight.data.cpu()\n",
    "\n",
    "for i, word in idx2word.items():\n",
    "    vec = embeddings[i].numpy().round(3)\n",
    "    print(f\"{word:10s}: {vec}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
