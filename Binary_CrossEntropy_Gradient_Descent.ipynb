{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399d0286",
   "metadata": {},
   "source": [
    "# Understanding Cross-Entropy Derivatives in Binary Classification\n",
    "This notebook shows how the scalar loss value is used in gradient descent by computing the derivative of the binary cross-entropy loss with respect to weights, using sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48a020",
   "metadata": {},
   "source": [
    "## Step 1: Setup Inputs, Weights, and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inputs\n",
    "x1, x2 = 2.0, 3.0\n",
    "x = np.array([x1, x2])\n",
    "\n",
    "# Initial weights and bias\n",
    "w = np.array([0.5, -0.4])\n",
    "b = 0.0\n",
    "\n",
    "# Target label\n",
    "y = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26e6a2",
   "metadata": {},
   "source": [
    "## Step 2: Forward Pass (Logit, Sigmoid Output, Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward pass\n",
    "z = np.dot(w, x) + b\n",
    "y_hat = sigmoid(z)\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "loss = - (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "print(f\"z (logit): {z}\")\n",
    "print(f\"ŷ (sigmoid output): {y_hat}\")\n",
    "print(f\"Loss (cross-entropy): {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c55184",
   "metadata": {},
   "source": [
    "## Step 3: Backpropagation — Compute Gradient of Loss w.r.t. Each Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c69904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of loss with respect to y_hat\n",
    "dL_dyhat = - (y / y_hat) + ((1 - y) / (1 - y_hat))\n",
    "\n",
    "# Derivative of y_hat with respect to z\n",
    "dyhat_dz = y_hat * (1 - y_hat)\n",
    "\n",
    "# Derivative of z with respect to each weight w_i is x_i\n",
    "dz_dw = x  # because z = w1*x1 + w2*x2\n",
    "\n",
    "# Chain rule: dL/dw = dL/dyhat * dyhat/dz * dz/dw\n",
    "dL_dz = dL_dyhat * dyhat_dz\n",
    "grad_w = dL_dz * dz_dw  # gradient vector\n",
    "grad_b = dL_dz          # gradient for bias\n",
    "\n",
    "print(f\"dL/dz: {dL_dz}\")\n",
    "print(f\"Gradient w.r.t. weights (dL/dw): {grad_w}\")\n",
    "print(f\"Gradient w.r.t. bias (dL/db): {grad_b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c836da1",
   "metadata": {},
   "source": [
    "## Step 4: Update Weights with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Gradient descent update\n",
    "w_new = w - lr * grad_w\n",
    "b_new = b - lr * grad_b\n",
    "\n",
    "print(f\"Updated weights: {w_new}\")\n",
    "print(f\"Updated bias: {b_new}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686fed9",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "- Even though the loss is a scalar, its derivative w.r.t. weights tells us how to change weights to reduce it.\n",
    "- We used chain rule to connect loss → prediction → weights.\n",
    "- This is the foundation of how neural networks learn through backpropagation."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
